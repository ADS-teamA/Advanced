"""
Test script for prompt batching optimization.

Measures speedup from batching multiple sections in one API call.
"""

import time
import logging
from pathlib import Path

from src.summary_generator import SummaryGenerator
from src.config import SummarizationConfig

# Setup logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def generate_test_sections(num_sections: int = 30) -> list[tuple[str, str]]:
    """Generate test sections for benchmarking."""
    test_sections = []

    base_texts = [
        "This section describes the waste disposal requirements for organizations. "
        "It includes information about hazardous waste management, non-hazardous waste handling, "
        "and reporting obligations for waste generated by operations.",
        "The standard requires organizations to track and report all waste generated by operations. "
        "This includes categorization by composition, disposal method, and hazard classification.",
        "Organizations must implement proper waste segregation systems to ensure compliance. "
        "This includes separate collection points for different waste categories and proper labeling.",
        "Regular audits of waste management practices are required to maintain certification. "
        "These audits should cover all aspects of waste handling from generation to disposal.",
        "Employee training on waste management procedures is mandatory for all staff. "
        "Training should be conducted annually and documented for compliance purposes.",
    ]

    for i in range(num_sections):
        text = base_texts[i % len(base_texts)]
        title = f"Section {i+1}: Test Section"
        test_sections.append((text, title))

    return test_sections


def test_parallel_mode(num_sections: int = 30):
    """Test original parallel mode (one API call per section)."""
    logger.info(f"\n{'='*80}")
    logger.info("Testing PARALLEL MODE (original implementation)")
    logger.info(f"{'='*80}")

    # Create config with parallel mode
    config = SummarizationConfig(enable_prompt_batching=False, max_workers=20)  # Disable batching

    generator = SummaryGenerator(config)
    sections = generate_test_sections(num_sections)

    logger.info(f"Generating summaries for {num_sections} sections...")
    start_time = time.time()

    summaries = generator.generate_batch_summaries(sections)

    elapsed_time = time.time() - start_time

    logger.info(f"‚úì Generated {len(summaries)} summaries")
    logger.info(f"‚è±Ô∏è  Time: {elapsed_time:.2f} seconds")
    logger.info(f"üìä Average: {elapsed_time / len(summaries):.3f} seconds per summary")

    return elapsed_time, summaries


def test_batching_mode(num_sections: int = 30, batch_size: int = 15):
    """Test new prompt batching mode (multiple sections per API call)."""
    logger.info(f"\n{'='*80}")
    logger.info("Testing PROMPT BATCHING MODE (optimized)")
    logger.info(f"{'='*80}")

    # Create config with batching enabled
    config = SummarizationConfig(
        enable_prompt_batching=True, batch_size=batch_size  # Enable batching
    )

    generator = SummaryGenerator(config)
    sections = generate_test_sections(num_sections)

    num_api_calls = (num_sections + batch_size - 1) // batch_size
    logger.info(f"Generating summaries for {num_sections} sections...")
    logger.info(f"Expected API calls: {num_api_calls} (batch_size={batch_size})")

    start_time = time.time()

    summaries = generator.generate_batch_summaries(sections)

    elapsed_time = time.time() - start_time

    logger.info(f"‚úì Generated {len(summaries)} summaries")
    logger.info(f"‚è±Ô∏è  Time: {elapsed_time:.2f} seconds")
    logger.info(f"üìä Average: {elapsed_time / len(summaries):.3f} seconds per summary")

    return elapsed_time, summaries


def main():
    """Run benchmark comparison."""
    num_sections = 30  # Test with 30 sections

    logger.info("=" * 80)
    logger.info("BENCHMARK: Prompt Batching Optimization")
    logger.info("=" * 80)
    logger.info(f"Test size: {num_sections} sections")
    logger.info("")

    try:
        # Test parallel mode (baseline)
        parallel_time, parallel_summaries = test_parallel_mode(num_sections)

        # Test batching mode (optimized)
        batching_time, batching_summaries = test_batching_mode(num_sections, batch_size=15)

        # Calculate speedup
        speedup = parallel_time / batching_time if batching_time > 0 else 0

        logger.info(f"\n{'='*80}")
        logger.info("RESULTS")
        logger.info(f"{'='*80}")
        logger.info(f"Parallel mode:  {parallel_time:.2f}s ({len(parallel_summaries)} summaries)")
        logger.info(f"Batching mode:  {batching_time:.2f}s ({len(batching_summaries)} summaries)")
        logger.info(f"Speedup:        {speedup:.2f}√ó faster")
        logger.info(f"Time saved:     {parallel_time - batching_time:.2f}s")
        logger.info("")

        if speedup >= 10:
            logger.info("üéâ EXCELLENT! Achieved 10√ó+ speedup")
        elif speedup >= 5:
            logger.info("‚úÖ GOOD! Achieved 5-10√ó speedup")
        elif speedup >= 2:
            logger.info("üëç OK! Achieved 2-5√ó speedup")
        else:
            logger.warning("‚ö†Ô∏è  Speedup lower than expected")

        logger.info("")
        logger.info("Sample summaries (first 3):")
        for i in range(min(3, len(batching_summaries))):
            summary = batching_summaries[i]
            logger.info(f"  [{i+1}] ({len(summary)} chars): {summary[:100]}...")

    except Exception as e:
        logger.error(f"Benchmark failed: {e}", exc_info=True)
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
