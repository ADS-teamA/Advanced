"""
Test OpenAI Batch API implementation for summary generation.

Compares:
1. Parallel mode (baseline) - 20 concurrent API calls
2. Batch API mode (optimized) - Single batch job, 50% cost savings
"""

import time
import logging
from pathlib import Path

from src.summary_generator import SummaryGenerator
from src.config import SummarizationConfig
from src.cost_tracker import get_global_tracker, reset_global_tracker

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def generate_test_sections(num_sections: int = 30) -> list[tuple[str, str]]:
    """Generate test sections for benchmarking."""
    test_sections = []

    base_texts = [
        "This section describes the waste disposal requirements for organizations. "
        "It includes information about hazardous waste management, non-hazardous waste handling, "
        "and reporting obligations for waste generated by operations.",

        "The standard requires organizations to track and report all waste generated by operations. "
        "This includes categorization by composition, disposal method, and hazard classification.",

        "Organizations must implement proper waste segregation systems to ensure compliance. "
        "This includes separate collection points for different waste categories and proper labeling.",

        "Regular audits of waste management practices are required to maintain certification. "
        "These audits should cover all aspects of waste handling from generation to disposal.",

        "Employee training on waste management procedures is mandatory for all staff. "
        "Training should be conducted annually and documented for compliance purposes.",
    ]

    for i in range(num_sections):
        text = base_texts[i % len(base_texts)]
        title = f"Section {i+1}: Test Section"
        test_sections.append((text, title))

    return test_sections


def test_parallel_mode(num_sections: int = 30):
    """Test parallel mode (baseline)."""
    logger.info(f"\n{'='*80}")
    logger.info("TEST 1: PARALLEL MODE (baseline)")
    logger.info(f"{'='*80}")

    # Reset cost tracker
    reset_global_tracker()

    # Create config with parallel mode
    config = SummarizationConfig(
        use_batch_api=False,  # Disable batch API
        max_workers=20
    )

    generator = SummaryGenerator(config)
    sections = generate_test_sections(num_sections)

    logger.info(f"Generating summaries for {num_sections} sections...")
    start_time = time.time()

    summaries = generator.generate_batch_summaries(sections)

    elapsed_time = time.time() - start_time

    # Get cost tracker
    tracker = get_global_tracker()
    total_cost = tracker.get_total_cost()

    logger.info(f"✓ Generated {len(summaries)} summaries")
    logger.info(f"⏱️  Time: {elapsed_time:.2f} seconds")
    logger.info(f"💰 Cost: ${total_cost:.6f}")
    logger.info(f"📊 Average: {elapsed_time / len(summaries):.3f}s per summary")
    logger.info(f"📊 Cost per summary: ${total_cost / len(summaries):.6f}")

    return elapsed_time, summaries, total_cost


def test_batch_api_mode(num_sections: int = 30):
    """Test OpenAI Batch API mode (50% cheaper)."""
    logger.info(f"\n{'='*80}")
    logger.info("TEST 2: BATCH API MODE (50% cost savings)")
    logger.info(f"{'='*80}")

    # Reset cost tracker
    reset_global_tracker()

    # Create config with batch API enabled
    config = SummarizationConfig(
        use_batch_api=True,  # Enable batch API
        batch_api_poll_interval=10,  # Poll every 10s for testing
        batch_api_timeout=600  # 10 minute timeout
    )

    generator = SummaryGenerator(config)
    sections = generate_test_sections(num_sections)

    logger.info(f"Generating summaries for {num_sections} sections...")
    logger.info("NOTE: Batch API is async, will take 1-5 minutes to complete")

    start_time = time.time()

    summaries = generator.generate_batch_summaries(sections)

    elapsed_time = time.time() - start_time

    # Get cost tracker
    tracker = get_global_tracker()
    total_cost = tracker.get_total_cost()

    logger.info(f"✓ Generated {len(summaries)} summaries")
    logger.info(f"⏱️  Time: {elapsed_time:.2f} seconds (includes polling)")
    logger.info(f"💰 Cost: ${total_cost:.6f}")
    logger.info(f"📊 Average: {elapsed_time / len(summaries):.3f}s per summary")
    logger.info(f"📊 Cost per summary: ${total_cost / len(summaries):.6f}")

    return elapsed_time, summaries, total_cost


def main():
    """Run benchmark comparison."""
    num_sections = 30  # Test with 30 sections

    logger.info("="*80)
    logger.info("BENCHMARK: OpenAI Batch API vs Parallel Mode")
    logger.info("="*80)
    logger.info(f"Test size: {num_sections} sections")
    logger.info("")

    try:
        # Test 1: Parallel mode (baseline)
        parallel_time, parallel_summaries, parallel_cost = test_parallel_mode(num_sections)

        # Test 2: Batch API mode (optimized)
        batch_time, batch_summaries, batch_cost = test_batch_api_mode(num_sections)

        # Calculate savings
        cost_savings = parallel_cost - batch_cost
        cost_savings_pct = (cost_savings / parallel_cost * 100) if parallel_cost > 0 else 0
        time_diff = batch_time - parallel_time

        logger.info(f"\n{'='*80}")
        logger.info("RESULTS")
        logger.info(f"{'='*80}")
        logger.info(f"Parallel mode:  {parallel_time:.2f}s, ${parallel_cost:.6f}")
        logger.info(f"Batch API mode: {batch_time:.2f}s, ${batch_cost:.6f}")
        logger.info("")
        logger.info(f"💰 Cost savings: ${cost_savings:.6f} ({cost_savings_pct:.1f}% cheaper)")
        logger.info(f"⏱️  Time difference: {time_diff:+.2f}s")
        logger.info("")

        if cost_savings_pct >= 40:
            logger.info("🎉 EXCELLENT! Achieved 40%+ cost savings")
        elif cost_savings_pct >= 30:
            logger.info("✅ GOOD! Achieved 30-40% cost savings")
        elif cost_savings_pct >= 20:
            logger.info("👍 OK! Achieved 20-30% cost savings")
        else:
            logger.warning("⚠️  Cost savings lower than expected")

        logger.info("")
        logger.info("Sample summaries (first 3):")
        for i in range(min(3, len(batch_summaries))):
            summary = batch_summaries[i]
            logger.info(f"  [{i+1}] ({len(summary)} chars): {summary[:100]}...")

        logger.info("")
        logger.info("💡 Recommendation: Use Batch API for document indexing (async OK)")
        logger.info("💡 Use parallel mode only when real-time response needed")

    except Exception as e:
        logger.error(f"Benchmark failed: {e}", exc_info=True)
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
