# ====================================================================
# MY_SUJBOT - RAG Pipeline Configuration
# ====================================================================
# See INSTALL.md for platform-specific setup instructions

# ====================================================================
# LLM CONFIGURATION (for summaries and RAG agent)
# ====================================================================
# Provider: "claude" or "openai"
LLM_PROVIDER=claude

# Model selection based on provider:
# Claude: "claude-sonnet-4-5-20250929", "claude-haiku-4-5-20251001" (or "sonnet", "haiku")
# OpenAI GPT-4: "gpt-4o", "gpt-4o-mini"
# OpenAI GPT-5: "gpt-5", "gpt-5-mini", "gpt-5-nano", "gpt-5-pro"
# OpenAI O-series: "o1", "o1-mini", "o3", "o3-mini", "o3-pro", "o4-mini"
LLM_MODEL=claude-sonnet-4-5-20250929

# ====================================================================
# REQUIRED: LLM API Keys
# ====================================================================
# At least ONE of these is required for PHASE 2 (summary generation)

# Anthropic Claude (required if LLM_PROVIDER=claude)
ANTHROPIC_API_KEY=sk-ant-your_key_here
# Get your key: https://console.anthropic.com/

# OpenAI (required if LLM_PROVIDER=openai or using OpenAI embeddings)
OPENAI_API_KEY=sk-your_key_here
# Get your key: https://platform.openai.com/api-keys

# ====================================================================
# EMBEDDING MODEL SELECTION (PHASE 4)
# ====================================================================
# Provider: "huggingface", "voyage", or "openai"
EMBEDDING_PROVIDER=huggingface

# Model selection based on provider and platform:
# See INSTALL.md for detailed comparison and recommendations
EMBEDDING_MODEL=bge-m3

# Option 1: HuggingFace BGE-M3 (FREE, LOCAL, BEST FOR APPLE SILICON)
# - Runs locally, no API costs, multilingual, Czech support
# - GPU-accelerated on Apple Silicon (M1/M2/M3) and NVIDIA GPUs
# - Requires: uv pip install sentence-transformers
# EMBEDDING_PROVIDER=huggingface
# EMBEDDING_MODEL=bge-m3

# Option 2: OpenAI (RECOMMENDED FOR WINDOWS)
# - Works on all platforms, no local installation issues
# - High quality, fast, requires OPENAI_API_KEY
# EMBEDDING_PROVIDER=openai
# EMBEDDING_MODEL=text-embedding-3-large

# Option 3: Voyage AI (BEST QUALITY, ALL PLATFORMS)
# - SOTA performance (#1 MLEB 2025 benchmark)
# - Legal/technical document optimized models available
# - Requires VOYAGE_API_KEY
# EMBEDDING_PROVIDER=voyage
# EMBEDDING_MODEL=voyage-3-large

# Voyage AI API key (required if EMBEDDING_PROVIDER=voyage)
VOYAGE_API_KEY=
# Get your key: https://www.voyageai.com/

# ====================================================================
# OPTIONAL: Knowledge Graph Configuration (PHASE 5A)
# ====================================================================
# LLM provider for entity/relationship extraction
KG_LLM_PROVIDER=openai
KG_LLM_MODEL=gpt-4o-mini

# Storage backend: "simple" (JSON), "neo4j", or "networkx"
KG_BACKEND=simple

# Export path for simple backend
# KG_EXPORT_PATH=./data/graphs/knowledge_graph.json

# Enable verbose logging for KG extraction
# KG_VERBOSE=true

# Neo4j Configuration (if KG_BACKEND=neo4j)
# NEO4J_URI=bolt://localhost:7687
# NEO4J_USERNAME=neo4j
# NEO4J_PASSWORD=your_password
# NEO4J_DATABASE=neo4j

# ====================================================================
# OPTIONAL: RAG Agent Configuration (PHASE 7)
# ====================================================================
# Agent model (can differ from LLM_MODEL for summaries)
# AGENT_MODEL=claude-sonnet-4-5-20250929

# Vector store path
# VECTOR_STORE_PATH=output/hybrid_store

# Knowledge graph path (if using KG with agent)
# KNOWLEDGE_GRAPH_PATH=output/knowledge_graph.json

# Query enhancement features
# ENABLE_HYDE=false              # Hypothetical Document Embeddings
# ENABLE_DECOMPOSITION=false     # Query decomposition for complex queries

# ====================================================================
# OPTIONAL: Pipeline Configuration
# ====================================================================
# Directory paths (defaults used if not set)
# DATA_DIR=data
# OUTPUT_DIR=output

# Processing options (research-optimized defaults)
# CHUNK_SIZE=500              # Characters per chunk
# ENABLE_SAC=true             # Summary-Augmented Chunking
# ENABLE_SMART_HIERARCHY=true # Font-size based hierarchy detection
# SUMMARY_MAX_CHARS=150       # Summary length

# ====================================================================
# PLATFORM-SPECIFIC RECOMMENDATIONS
# ====================================================================
#
# WINDOWS:
#   LLM_PROVIDER=claude (or openai)
#   EMBEDDING_PROVIDER=openai
#   EMBEDDING_MODEL=text-embedding-3-large
#   Why: Avoids PyTorch/DLL issues with local models
#
# macOS (Apple Silicon M1/M2/M3):
#   LLM_PROVIDER=claude (recommended for quality)
#   EMBEDDING_PROVIDER=huggingface
#   EMBEDDING_MODEL=bge-m3
#   Why: FREE GPU acceleration via MPS, excellent performance
#
# macOS (Intel):
#   LLM_PROVIDER=claude (or openai)
#   EMBEDDING_PROVIDER=openai
#   EMBEDDING_MODEL=text-embedding-3-large
#   Why: Avoid slow CPU inference with local models
#
# Linux (NVIDIA GPU):
#   LLM_PROVIDER=claude (or openai)
#   EMBEDDING_PROVIDER=huggingface
#   EMBEDDING_MODEL=bge-m3
#   Why: FREE GPU acceleration via CUDA
#
# Linux (CPU only):
#   LLM_PROVIDER=claude (or openai)
#   EMBEDDING_PROVIDER=openai
#   EMBEDDING_MODEL=text-embedding-3-large
#   Why: Avoid slow CPU inference with local models
#
# For BEST QUALITY (all platforms):
#   EMBEDDING_PROVIDER=voyage
#   EMBEDDING_MODEL=voyage-3-large (or kanon-2)
#   Note: Requires VOYAGE_API_KEY, costs money but best results
#
# See INSTALL.md for detailed setup instructions for your platform.
