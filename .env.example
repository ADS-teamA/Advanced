# ====================================================================
# MY_SUJBOT - RAG Pipeline Configuration
# ====================================================================
# See INSTALL.md for platform-specific setup instructions
#
# CONFIGURATION PHILOSOPHY:
# - .env file (this file) = API keys + Model selection
# - src/config.py = Research-backed parameters (chunk_size=500, temperature=0.3, etc.)
#
# This file is the SINGLE SOURCE OF TRUTH for:
#   ✅ API keys (never commit these!)
#   ✅ Which LLM to use (claude vs openai, which model)
#   ✅ Which embedding model to use (bge-m3 vs text-embedding-3-large)
#
# Research parameters (chunk size, temperature, etc.) are in src/config.py
# and should rarely need changing.

# ====================================================================
# LLM CONFIGURATION (for summaries and RAG agent)
# ====================================================================
# Provider: "claude" or "openai"
LLM_PROVIDER=claude

# Model selection based on provider:
# Claude: "claude-sonnet-4-5-20250929", "claude-haiku-4-5-20251001" (or "sonnet", "haiku")
# OpenAI GPT-4: "gpt-4o", "gpt-4o-mini"
# OpenAI GPT-5: "gpt-5", "gpt-5-mini", "gpt-5-nano", "gpt-5-pro"
# OpenAI O-series: "o1", "o1-mini", "o3", "o3-mini", "o3-pro", "o4-mini"
LLM_MODEL=claude-sonnet-4-5-20250929

# ====================================================================
# REQUIRED: LLM API Keys
# ====================================================================
# At least ONE of these is required for PHASE 2 (summary generation)

# Anthropic Claude (required if LLM_PROVIDER=claude)
ANTHROPIC_API_KEY=sk-ant-your_key_here
# Get your key: https://console.anthropic.com/

# OpenAI (required if LLM_PROVIDER=openai or using OpenAI embeddings)
OPENAI_API_KEY=sk-your_key_here
# Get your key: https://platform.openai.com/api-keys

# ====================================================================
# EMBEDDING MODEL SELECTION (PHASE 4)
# ====================================================================
# Provider: "huggingface", "voyage", or "openai"
EMBEDDING_PROVIDER=huggingface

# Model selection based on provider and platform:
# See INSTALL.md for detailed comparison and recommendations
EMBEDDING_MODEL=bge-m3

# Option 1: HuggingFace BGE-M3 (FREE, LOCAL, BEST FOR APPLE SILICON)
# - Runs locally, no API costs, multilingual, Czech support
# - GPU-accelerated on Apple Silicon (M1/M2/M3) and NVIDIA GPUs
# - Requires: uv pip install sentence-transformers
# EMBEDDING_PROVIDER=huggingface
# EMBEDDING_MODEL=bge-m3

# Option 2: OpenAI (RECOMMENDED FOR WINDOWS)
# - Works on all platforms, no local installation issues
# - High quality, fast, requires OPENAI_API_KEY
# EMBEDDING_PROVIDER=openai
# EMBEDDING_MODEL=text-embedding-3-large

# Option 3: Voyage AI (BEST QUALITY, ALL PLATFORMS)
# - SOTA performance (#1 MLEB 2025 benchmark)
# - Legal/technical document optimized models available
# - Requires VOYAGE_API_KEY
# EMBEDDING_PROVIDER=voyage
# EMBEDDING_MODEL=voyage-3-large

# Voyage AI API key (required if EMBEDDING_PROVIDER=voyage)
VOYAGE_API_KEY=
# Get your key: https://www.voyageai.com/

# ====================================================================
# OPTIONAL: Embedding Configuration (PHASE 4)
# ====================================================================
# Batch size for embedding generation (default: 64)
# EMBEDDING_BATCH_SIZE=64

# Enable embedding cache for faster query processing (default: true)
# Cache provides 40-80% hit rate, reducing latency by 100-200ms
# EMBEDDING_CACHE_ENABLED=true

# Maximum cache size (default: 1000 entries)
# EMBEDDING_CACHE_SIZE=1000

# ====================================================================
# OPTIONAL: Pipeline Configuration
# ====================================================================
# Speed/Cost Mode: "fast" (2-3 min, full price) or "eco" (15-30 min, 50% cheaper)
# Eco mode uses OpenAI Batch API for PHASE 2 (summaries) and PHASE 3 (SAC contexts)
# SPEED_MODE=fast

# OCR Language codes (comma-separated, default: ces,eng)
# See https://tesseract-ocr.github.io/tessdoc/Data-Files-in-different-versions.html
# OCR_LANGUAGE=ces,eng

# Enable font-size based hierarchy detection (default: true)
# ENABLE_SMART_HIERARCHY=true

# Enable Knowledge Graph construction (default: true for SOTA 2025)
# ENABLE_KNOWLEDGE_GRAPH=true

# Enable Hybrid Search (BM25 + Dense + RRF fusion, default: true)
# ENABLE_HYBRID_SEARCH=true

# ====================================================================
# OPTIONAL: Knowledge Graph Configuration (PHASE 5A)
# ====================================================================
# LLM provider for entity/relationship extraction
KG_LLM_PROVIDER=openai
KG_LLM_MODEL=gpt-4o-mini

# Storage backend: "simple" (JSON), "neo4j", or "networkx"
KG_BACKEND=simple

# Export path for simple backend
# KG_EXPORT_PATH=./data/graphs/knowledge_graph.json

# Enable verbose logging for KG extraction
# KG_VERBOSE=true

# Neo4j Configuration (if KG_BACKEND=neo4j)
# NEO4J_URI=bolt://localhost:7687
# NEO4J_USERNAME=neo4j
# NEO4J_PASSWORD=your_password
# NEO4J_DATABASE=neo4j

# ====================================================================
# OPTIONAL: RAG Agent Configuration (PHASE 7)
# ====================================================================
# Agent model (can differ from LLM_MODEL for summaries)
# AGENT_MODEL=claude-sonnet-4-5-20250929

# Vector store path
# VECTOR_STORE_PATH=output/hybrid_store

# Knowledge graph path (if using KG with agent)
# KNOWLEDGE_GRAPH_PATH=output/knowledge_graph.json

# Query enhancement features
# ENABLE_HYDE=false              # Hypothetical Document Embeddings
# ENABLE_DECOMPOSITION=false     # Query decomposition for complex queries

# ====================================================================
# RESEARCH PARAMETERS (configured in src/config.py, not here)
# ====================================================================
# The following parameters are configured in src/config.py with research-backed
# defaults and should RARELY need changing:
#
# - CHUNK_SIZE=500 (RCTS optimal from LegalBench-RAG research)
# - ENABLE_SAC=true (Summary-Augmented Chunking, -58% error rate)
# - ENABLE_SMART_HIERARCHY=true (Font-size based hierarchy detection)
# - SUMMARY_MAX_CHARS=150 (Generic summary optimal length)
# - TEMPERATURE=0.3 (Low for consistency)
# - BATCH_SIZE=32 (Embedding generation)
#
# To change these, edit src/config.py classes directly.
# DO NOT add these to .env file - they belong in config.py!

# ====================================================================
# PLATFORM-SPECIFIC RECOMMENDATIONS
# ====================================================================
#
# WINDOWS:
#   LLM_PROVIDER=claude (or openai)
#   EMBEDDING_PROVIDER=openai
#   EMBEDDING_MODEL=text-embedding-3-large
#   Why: Avoids PyTorch/DLL issues with local models
#
# macOS (Apple Silicon M1/M2/M3):
#   LLM_PROVIDER=claude (recommended for quality)
#   EMBEDDING_PROVIDER=huggingface
#   EMBEDDING_MODEL=bge-m3
#   Why: FREE GPU acceleration via MPS, excellent performance
#
# macOS (Intel):
#   LLM_PROVIDER=claude (or openai)
#   EMBEDDING_PROVIDER=openai
#   EMBEDDING_MODEL=text-embedding-3-large
#   Why: Avoid slow CPU inference with local models
#
# Linux (NVIDIA GPU):
#   LLM_PROVIDER=claude (or openai)
#   EMBEDDING_PROVIDER=huggingface
#   EMBEDDING_MODEL=bge-m3
#   Why: FREE GPU acceleration via CUDA
#
# Linux (CPU only):
#   LLM_PROVIDER=claude (or openai)
#   EMBEDDING_PROVIDER=openai
#   EMBEDDING_MODEL=text-embedding-3-large
#   Why: Avoid slow CPU inference with local models
#
# For BEST QUALITY (all platforms):
#   EMBEDDING_PROVIDER=voyage
#   EMBEDDING_MODEL=voyage-3-large (or kanon-2)
#   Note: Requires VOYAGE_API_KEY, costs money but best results
#
# See INSTALL.md for detailed setup instructions for your platform.
