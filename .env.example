# ====================================================================
# MY_SUJBOT - RAG Pipeline Configuration
# ====================================================================
# See INSTALL.md for platform-specific setup instructions

# ====================================================================
# REQUIRED: LLM API Key (for PHASE 2: Summary Generation)
# ====================================================================
# At least ONE of these is required:

# Anthropic Claude (recommended for summaries)
ANTHROPIC_API_KEY=sk-ant-your_key_here
# Get your key: https://console.anthropic.com/

# OpenAI (alternative for summaries, also used for embeddings if selected)
OPENAI_API_KEY=sk-your_key_here
# Get your key: https://platform.openai.com/api-keys

# ====================================================================
# EMBEDDING MODEL SELECTION (PHASE 4)
# ====================================================================
# Choose ONE embedding model based on your platform and needs
# See INSTALL.md for detailed comparison and recommendations

# Option 1: OpenAI (RECOMMENDED FOR WINDOWS)
# - Works on all platforms, no local installation issues
# - High quality, fast, requires OPENAI_API_KEY
EMBEDDING_MODEL=text-embedding-3-large

# Option 2: Voyage AI (BEST QUALITY, ALL PLATFORMS)
# - SOTA performance, requires VOYAGE_API_KEY
# - Legal/technical document optimized models available
# EMBEDDING_MODEL=voyage-3-large
# VOYAGE_API_KEY=your_voyage_key_here
# Get your key: https://www.voyageai.com/

# Option 3: BGE-M3 Local (FREE, BEST FOR APPLE SILICON)
# - Runs locally, no API costs, multilingual
# - GPU-accelerated on Apple Silicon (M1/M2/M3)
# - Requires sentence-transformers: uv pip install sentence-transformers
# EMBEDDING_MODEL=bge-m3

# Option 4: Voyage Legal-Optimized (for legal documents)
# EMBEDDING_MODEL=voyage-law-2

# ====================================================================
# OPTIONAL: Knowledge Graph Configuration (PHASE 5A)
# ====================================================================
ENABLE_KNOWLEDGE_GRAPH=false

# If enabled, choose LLM provider for entity/relationship extraction
# KG_LLM_PROVIDER=openai    # or "anthropic"
# KG_LLM_MODEL=gpt-4o-mini  # or "claude-haiku"
# KG_BACKEND=simple         # simple, neo4j, or networkx

# Neo4j Configuration (if using Neo4j backend)
# NEO4J_URI=bolt://localhost:7687
# NEO4J_USERNAME=neo4j
# NEO4J_PASSWORD=your_password

# ====================================================================
# TASK-SPECIFIC LLM CONFIGURATION (NEW!)
# ====================================================================
# Configure different models for different tasks in the RAG pipeline
# This allows fine-grained control over costs and quality trade-offs

# Summary Generation (PHASE 2)
# SUMMARY_LLM_PROVIDER=claude
# SUMMARY_LLM_MODEL=haiku
# SUMMARY_LLM_TEMP=0.3

# Context Generation (PHASE 3 - Contextual Retrieval)
# CONTEXT_LLM_PROVIDER=claude
# CONTEXT_LLM_MODEL=haiku
# CONTEXT_LLM_TEMP=0.3

# Entity Extraction (PHASE 5A - Knowledge Graph)
# ENTITY_LLM_PROVIDER=openai
# ENTITY_LLM_MODEL=gpt-4o-mini
# ENTITY_LLM_TEMP=0.0

# Relationship Extraction (PHASE 5A - Knowledge Graph)
# RELATIONSHIP_LLM_PROVIDER=openai
# RELATIONSHIP_LLM_MODEL=gpt-4o-mini
# RELATIONSHIP_LLM_TEMP=0.0

# Agent (PHASE 7 - RAG Agent)
# AGENT_LLM_PROVIDER=claude
# AGENT_LLM_MODEL=sonnet
# AGENT_LLM_TEMP=0.3

# Query Decomposition (PHASE 7)
# QUERY_DECOMP_LLM_PROVIDER=claude
# QUERY_DECOMP_LLM_MODEL=haiku
# QUERY_DECOMP_LLM_TEMP=0.3

# HyDE (Hypothetical Document Embeddings, PHASE 7)
# HYDE_LLM_PROVIDER=claude
# HYDE_LLM_MODEL=haiku
# HYDE_LLM_TEMP=0.5

# ====================================================================
# OPTIONAL: Pipeline Configuration
# ====================================================================

# Directory paths (defaults used if not set)
# DATA_DIR=data
# OUTPUT_DIR=output

# Processing options
# CHUNK_SIZE=500              # Characters per chunk (research-optimized)
# ENABLE_SAC=true             # Summary-Augmented Chunking
# ENABLE_SMART_HIERARCHY=true # Font-size based hierarchy detection
# SUMMARY_MAX_CHARS=150       # Summary length

# ====================================================================
# PLATFORM-SPECIFIC NOTES
# ====================================================================
#
# WINDOWS:
#   - Recommended: text-embedding-3-large (avoid PyTorch issues)
#   - PyTorch must be installed separately (see INSTALL.md)
#
# macOS (Apple Silicon):
#   - Recommended: bge-m3 (FREE GPU acceleration via MPS)
#   - Alternative: text-embedding-3-large
#
# macOS (Intel):
#   - Recommended: text-embedding-3-large (avoid slow CPU inference)
#
# Linux (NVIDIA GPU):
#   - Recommended: bge-m3 (FREE GPU acceleration)
#
# Linux (CPU only):
#   - Recommended: text-embedding-3-large (avoid slow CPU inference)
#
# See INSTALL.md for detailed setup instructions for your platform.
