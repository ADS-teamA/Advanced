# ====================================================================
# MY_SUJBOT - RAG Pipeline Configuration
# ====================================================================
# See INSTALL.md for platform-specific setup instructions

# ====================================================================
# REQUIRED: LLM API Keys
# ====================================================================
# At least ONE of these is required for PHASE 2 (summary generation)

# Anthropic Claude (required if LLM_PROVIDER=claude)
ANTHROPIC_API_KEY=
# Get your key: https://console.anthropic.com/

# OpenAI (required if LLM_PROVIDER=openai or using OpenAI embeddings)
OPENAI_API_KEY=
# Get your key: https://platform.openai.com/api-keys

# ====================================================================
# LLM CONFIGURATION (for summaries and RAG agent)
# ====================================================================
# Provider: "claude" or "openai"
LLM_PROVIDER=openai

# Model selection based on provider:
# Claude: "claude-sonnet-4-5-20250929", "claude-haiku-4-5-20251001" (or "sonnet", "haiku")
# OpenAI GPT-4: "gpt-4o", "gpt-4o-mini"
# OpenAI GPT-5: "gpt-5", "gpt-5-mini", "gpt-5-nano", "gpt-5-pro"
# OpenAI O-series: "o1", "o1-mini", "o3", "o3-mini", "o3-pro", "o4-mini"
# OPTIMIZED: gpt-4o-mini je 40% levnější než gpt-5-nano ($0.15/$0.60 vs $0.25/$1.00)
LLM_MODEL=gpt-5-nano

# ====================================================================
# EMBEDDING MODEL SELECTION (PHASE 4)
# ====================================================================
# Provider: "huggingface", "voyage", or "openai"
# OPTIMIZED: BGE-M3 local embeddings = 100% úspora (ZDARMA)
# EMBEDDING_PROVIDER=huggingface
# EMBEDDING_MODEL=bge-m3

# Model selection based on provider and platform:
# See INSTALL.md for detailed comparison and recommendations

# Option 1: HuggingFace BGE-M3 (FREE, LOCAL, BEST FOR APPLE SILICON) ✅ ACTIVE
# - Runs locally, no API costs, multilingual, Czech support
# - GPU-accelerated on Apple Silicon (M1/M2/M3) and NVIDIA GPUs
# - Requires: uv pip install sentence-transformers

# Option 2: OpenAI (RECOMMENDED FOR WINDOWS)
# - Works on all platforms, no local installation issues
# - High quality, fast, requires OPENAI_API_KEY
# IMPORTANT: Must match the embedding model used during indexing!
EMBEDDING_PROVIDER=openai
EMBEDDING_MODEL=text-embedding-3-large

# Option 3: Voyage AI (BEST QUALITY, ALL PLATFORMS)
# - SOTA performance (#1 MLEB 2025 benchmark)
# - Legal/technical document optimized models available
# - Requires VOYAGE_API_KEY
# EMBEDDING_PROVIDER=voyage
# EMBEDDING_MODEL=voyage-3-large

# Voyage AI API key (required if EMBEDDING_PROVIDER=voyage)
VOYAGE_API_KEY=
# Get your key: https://www.voyageai.com/

# ====================================================================
# OPTIONAL: Knowledge Graph Configuration (PHASE 5A)
# ====================================================================
# LLM provider for entity/relationship extraction
# OPTIMIZED: gpt-4o-mini je 70% levnější než gpt-5-mini ($0.15/$0.60 vs $0.50/$2.00)
KG_LLM_PROVIDER=openai
KG_LLM_MODEL=gpt-5-mini

# Storage backend: "simple" (JSON), "neo4j", or "networkx"
KG_BACKEND=simple

# Export path for simple backend
KG_EXPORT_PATH=./output/graphs/knowledge_graph.json

# Enable verbose logging for KG extraction
KG_VERBOSE=true

# Neo4j Configuration (if KG_BACKEND=neo4j)
# NEO4J_URI=bolt://localhost:7687
# NEO4J_USERNAME=neo4j
# NEO4J_PASSWORD=your_password
# NEO4J_DATABASE=neo4j

# ====================================================================
# OPTIONAL: RAG Agent Configuration (PHASE 7)
# ====================================================================
# Agent model (can differ from LLM_MODEL for summaries)
AGENT_MODEL=claude-haiku-4-5

# Vector store path (must point to actual phase4_vector_store directory)
VECTOR_STORE_PATH=vector_db

# Knowledge graph path (if using KG with agent)
KNOWLEDGE_GRAPH_PATH=output/knowledge_graph.json

# Note: HyDE and query decomposition have been removed as of January 2025
# These features were removed in favor of simpler, more efficient approaches:
# - Claude SDK's agentic tool use handles complex queries naturally
# - Prompt caching provides better cost-efficiency than query decomposition

# Query expansion (new unified search tool)
# LLM model for query expansion - gpt-4o-mini recommended for stability
# Provider is auto-detected from QUERY_EXPANSION_MODEL name (gpt-* = openai, claude-* = anthropic)
# Options: gpt-4o-mini (stable, fast), gpt-5-nano (experimental, may have issues), claude-haiku-4-5 (Anthropic)
QUERY_EXPANSION_MODEL=gpt-4o-mini

# Prompt caching (Anthropic only, 90% cost reduction on cached tokens)
ENABLE_PROMPT_CACHING=true    # Cache system prompt, tools, and init messages

# Context management (prevents quadratic cost growth in long conversations)
ENABLE_CONTEXT_MANAGEMENT=true  # Prune old tool results, keep only Q&A
CONTEXT_MANAGEMENT_TRIGGER=50000  # Start pruning at 50K input tokens
CONTEXT_MANAGEMENT_KEEP=3         # Keep last 3 messages with full tool context

# ====================================================================
# OPTIONAL: Pipeline Configuration
# ====================================================================
# Directory paths (defaults used if not set)
DATA_DIR=data_test
OUTPUT_DIR=output

# Processing options (research-optimized defaults)
CHUNK_SIZE=500              # Characters per chunk
ENABLE_SAC=true             # Summary-Augmented Chunking
ENABLE_SMART_HIERARCHY=true # Font-size based hierarchy detection
SUMMARY_MAX_CHARS=150       # Summary length

# PHASE 5B: Hybrid Search (BM25 + Dense + RRF)
ENABLE_HYBRID_SEARCH=true   # Enable hybrid search (+23% precision)
HYBRID_FUSION_K=60          # RRF fusion parameter

# PHASE 5A: Knowledge Graph
ENABLE_KNOWLEDGE_GRAPH=true # Enable KG extraction
KG_MIN_ENTITY_CONFIDENCE=0.6
KG_MIN_RELATIONSHIP_CONFIDENCE=0.5