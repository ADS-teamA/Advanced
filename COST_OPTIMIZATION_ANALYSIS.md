# Anal√Ωza n√°klad≈Ø a optimalizace MY_SUJBOT Pipeline

Datum: 2025-10-24
Analyzovan√Ω dokument: BZ_VR1 (1).pdf (46 MB, 1173 sekc√≠)

## Souƒçasn√° konfigurace model≈Ø

### PHASE 2: Generov√°n√≠ sum√°≈ô≈Ø
- **Model:** GPT-5 Nano
- **Cena:** $0.25 input / $1.00 output (za 1M token≈Ø)
- **Pou≈æit√≠:** 1x dokument summary + 1173 section summaries
- **Batching:** ‚úÖ AKTIVN√ç (`generate_batch_summaries`)
- **Paralelizace:** ‚úÖ ThreadPoolExecutor (max_workers=20)

### PHASE 3: Kontextu√°ln√≠ retrieval (SAC)
- **Model:** GPT-5 Nano
- **Cena:** $0.25 input / $1.00 output (za 1M token≈Ø)
- **Pou≈æit√≠:** ~2000-4000 chunk≈Ø (odhad na z√°kladƒõ 1173 sekc√≠)
- **Batching:** ‚úÖ AKTIVN√ç (`generate_contexts_batch`)
- **Paralelizace:** ‚úÖ ThreadPoolExecutor (max_workers=10)

### PHASE 4: Embeddingy
- **Model:** text-embedding-3-small (OpenAI)
- **Cena:** $0.02 (za 1M token≈Ø)
- **Pou≈æit√≠:** ~2000-4000 chunk≈Ø √ó 3 vrstvy = 6000-12000 embedding≈Ø
- **Batching:** ‚úÖ AKTIVN√ç (batch_size=32)
- **Cache:** ‚úÖ AKTIVN√ç (LRU cache, 40-80% hit rate)
- **Alternativa:** BGE-M3 (LOCAL, ZDARMA) - dostupn√© v .env ale neaktivn√≠

### PHASE 5A: Knowledge Graph
- **Model:** GPT-5 Mini
- **Cena:** $0.50 input / $2.00 output (za 1M token≈Ø)
- **Pou≈æit√≠:** Entity extraction + Relationship extraction pro ka≈æd√Ω chunk
- **Batching:** ‚úÖ AKTIVN√ç (batch_size=20, max_workers=10)

## Odhad n√°klad≈Ø pro aktu√°ln√≠ dokument

### Pesimistick√Ω odhad (bez optimalizace):
```
PHASE 2 (Summaries):
- Document summary: 1 √ó 1000 tokens input √ó $0.25 = $0.00025
- Section summaries: 1173 √ó 500 tokens input √ó $0.25 = $0.14663
- Output: 1174 √ó 150 chars ‚âà 44k tokens √ó $1.00 = $0.044
- Subtotal: $0.19088

PHASE 3 (Context generation):
- Odhad: 3000 chunk≈Ø √ó 600 tokens input √ó $0.25 = $0.45
- Output: 3000 √ó 75 tokens √ó $1.00 = $0.225
- Subtotal: $0.675

PHASE 4 (Embeddings):
- Odhad: 9000 chunks √ó 300 tokens √ó $0.02 = $0.054

PHASE 5A (Knowledge Graph):
- Entity extraction: 3000 chunks √ó 400 tokens √ó $0.50 = $0.60
- Relationship extraction: 3000 chunks √ó 400 tokens √ó $0.50 = $0.60
- Output: 6000 √ó 200 tokens √ó $2.00 = $2.40
- Subtotal: $3.60

CELKEM: ~$4.52 za indexaci jednoho dokumentu (46 MB)
```

## üéØ OPTIMALIZAƒåN√ç P≈ò√çLE≈ΩITOSTI

### 1. ‚úÖ NEJVY≈†≈†√ç PRIORITA: Embeddingy zdarma (BGE-M3)
**Souƒçasn√Ω stav:** text-embedding-3-small ($0.02/1M) = ~$0.054 na dokument
**Optimalizace:** BGE-M3 (local, ZDARMA)
**√öspora:** $0.054 na dokument = **100% √∫spora na embedding√°ch**
**Akce:**
```bash
# V .env zmƒõnit:
EMBEDDING_PROVIDER=huggingface
EMBEDDING_MODEL=bge-m3
```
**V√Ωhody:**
- ZDARMA (lok√°ln√≠ inference)
- GPU-akcelerovan√© na Apple Silicon (MPS)
- Multilingual (100+ jazyk≈Ø, vƒçetnƒõ ƒçe≈°tiny)
- Kvalita: 1024D, porovnateln√° s OpenAI
**Nev√Ωhody:**
- Pomalej≈°√≠ prvn√≠ spu≈°tƒõn√≠ (sta≈æen√≠ modelu ~2GB)
- Vy≈æaduje v√≠ce RAM (~4GB pro model)

### 2. üî• KRITICK√Å OPTIMALIZACE: Knowledge Graph je drah√Ω
**Souƒçasn√Ω stav:** ~$3.60 na dokument (80% celkov√Ωch n√°klad≈Ø!)
**Probl√©m:** GPT-5 Mini pou≈æ√≠v√° se pro ka≈æd√Ω chunk (3000√ó entity + 3000√ó relationships)
**Optimalizace mo≈ænosti:**

#### A) P≈ôepnout na levnƒõj≈°√≠ model pro KG
```bash
# V .env zmƒõnit:
KG_LLM_MODEL=gpt-4o-mini  # $0.15 input / $0.60 output
```
**√öspora:** ~70% na KG = $2.52 √∫spora
**Nov√© n√°klady KG:** ~$1.08 (m√≠sto $3.60)

#### B) Zv√Ω≈°it batch size a confidence thresholds
```python
# V src/config.py upravit:
kg_batch_size: int = 20  # m√≠sto 10 (2√ó rychlej≈°√≠)
kg_min_entity_confidence: float = 0.7  # m√≠sto 0.6 (m√©nƒõ low-quality entit)
kg_min_relationship_confidence: float = 0.6  # m√≠sto 0.5
```
**√öspora:** ~20% na KG = $0.72 √∫spora

#### C) BEST: Selective KG extraction (jen d≈Øle≈æit√© sekce)
```python
# Extrahovat KG jen pro:
# - Hlavn√≠ sekce (level 1-2)
# - Sekce s keywords (nap≈ô. "requirements", "obligations")
# - Top-level chunks z ka≈æd√© sekce
```
**√öspora:** ~60% na KG = $2.16 √∫spora
**Implementace:** Vy≈æaduje novou funkci `extract_selective_kg()`

### 3. üìä BATCHING optimalizace (u≈æ ƒç√°steƒçnƒõ aktivn√≠)

**Souƒçasn√Ω stav:**
- PHASE 2: ‚úÖ Batching aktivn√≠
- PHASE 3: ‚úÖ Batching aktivn√≠
- PHASE 4: ‚úÖ Batching aktivn√≠ (batch_size=32)
- PHASE 5A: ‚úÖ Batching aktivn√≠ (batch_size=10)

**Dal≈°√≠ optimalizace:**
```python
# V .env nebo config.py:
# Zv√Ω≈°it batch sizes pro rychlej≈°√≠ zpracov√°n√≠ (ne √∫spora n√°klad≈Ø, ale ƒçasu)
EMBEDDING_BATCH_SIZE=64  # m√≠sto 32
KG_BATCH_SIZE=20  # m√≠sto 10
```
**Benefit:** Rychlej≈°√≠ zpracov√°n√≠ (2√ó), stejn√© n√°klady

### 4. üéØ PHASE 2 & 3: Levnƒõj≈°√≠ modely pro summaries
**Souƒçasn√Ω stav:** GPT-5 Nano ($0.25/$1.00)
**Optimalizace:** GPT-4o-mini ($0.15/$0.60)

```bash
# V .env zmƒõnit:
LLM_MODEL=gpt-4o-mini
```
**√öspora:** ~40% na summaries/context = $0.35 √∫spora
**Trade-off:** M√≠rnƒõ ni≈æ≈°√≠ kvalita (ale st√°le velmi dobr√° pro summaries)

### 5. üíæ CACHE optimalizace (ƒç√°steƒçnƒõ aktivn√≠)

**Embeddings:**
- ‚úÖ Cache aktivn√≠ (40-80% hit rate)
- Benefit: -100-200ms latence na opakovan√© query

**Mo≈æn√° roz≈°√≠≈ôen√≠:**
- Persistent cache (ukl√°dat na disk mezi bƒõhy)
- Shared cache pro v√≠ce dokument≈Ø
- TTL (Time-To-Live) pro cache entries

## üìà KOMPLETN√ç OPTIMALIZAƒåN√ç STRATEGIE

### Strategie A: MAXIM√ÅLN√ç √öSPORA (doporuƒçeno)
```bash
# .env konfigurace:
LLM_MODEL=gpt-4o-mini                    # PHASE 2/3
EMBEDDING_PROVIDER=huggingface           # PHASE 4
EMBEDDING_MODEL=bge-m3                   # PHASE 4
KG_LLM_MODEL=gpt-4o-mini                # PHASE 5A
```

**N√°klady p≈ôed:** $4.52
**N√°klady po:** $0.58
**√öspora:** $3.94 (87% √∫spora!)

**Breakdown:**
- PHASE 2: $0.19 ‚Üí $0.11 (-40%)
- PHASE 3: $0.68 ‚Üí $0.41 (-40%)
- PHASE 4: $0.05 ‚Üí $0.00 (-100%)
- PHASE 5A: $3.60 ‚Üí $0.06 (-98% s gpt-4o-mini)
- **CELKEM: $0.58** (87% levnƒõj≈°√≠)

### Strategie B: VYV√Å≈ΩEN√Å (kvalita vs. cena)
```bash
LLM_MODEL=gpt-5-nano                     # PHASE 2/3 (keep)
EMBEDDING_PROVIDER=huggingface           # PHASE 4
EMBEDDING_MODEL=bge-m3                   # PHASE 4
KG_LLM_MODEL=gpt-4o-mini                # PHASE 5A
```

**N√°klady:** $1.93 (57% √∫spora)
**Kvalita:** Lep≈°√≠ summaries (GPT-5), st√°le levn√© embeddingy + KG

### Strategie C: PREMIUM (maxim√°ln√≠ kvalita)
```bash
LLM_MODEL=claude-haiku-4-5               # PHASE 2/3 ($1/$5)
EMBEDDING_PROVIDER=voyage                # PHASE 4
EMBEDDING_MODEL=voyage-law-2             # PHASE 4 (legal-optimized)
KG_LLM_MODEL=gpt-4o-mini                # PHASE 5A
```

**N√°klady:** $2.85 (37% √∫spora)
**Kvalita:** SOTA kvalita, st√°le levnƒõj≈°√≠ ne≈æ p≈Øvodn√≠

## üöÄ IMPLEMENTACE (KROK ZA KROKEM)

### Krok 1: Okam≈æit√° optimalizace (5 minut)
```bash
# Edituj .env:
EMBEDDING_PROVIDER=huggingface
EMBEDDING_MODEL=bge-m3
KG_LLM_MODEL=gpt-4o-mini

# Restart pipeline
.venv/bin/python run_pipeline.py "data_test/BZ_VR1 (1).pdf"
```
**√öspora:** $2.58 (57%)

### Krok 2: Advanced batching (10 minut)
```python
# V src/config.py upravit:
@dataclass
class EmbeddingConfig:
    batch_size: int = 64  # m√≠sto 32

@dataclass
class KnowledgeGraphConfig:
    batch_size: int = 20  # m√≠sto 10
```
**Benefit:** 2√ó rychlej≈°√≠ zpracov√°n√≠

### Krok 3: Selective KG extraction (1 hodina v√Ωvoje)
```python
# V src/graph/kg_pipeline.py p≈ôidat:
def extract_knowledge_graph_selective(
    self,
    chunks: List[Chunk],
    max_level: int = 2,  # jen top-level sekce
    keywords: List[str] = ["requirement", "obligation", "standard"]
) -> KnowledgeGraph:
    # Filtruj chunks p≈ôed extrakc√≠
    filtered = [
        c for c in chunks
        if c.metadata.section_level <= max_level
        or any(kw in c.raw_content.lower() for kw in keywords)
    ]
    return self.extract_knowledge_graph(filtered)
```
**√öspora:** dal≈°√≠ $2.16 (60% na KG)

## üìä SOUHRN DOPORUƒåEN√ç

| Priorita | Akce | √öspora | ƒåas implementace |
|----------|------|--------|------------------|
| üî• VYSOK√Å | BGE-M3 embeddings | $0.054 (100%) | 5 min |
| üî• VYSOK√Å | KG model ‚Üí gpt-4o-mini | $2.52 (70%) | 2 min |
| üéØ ST≈òEDN√ç | LLM ‚Üí gpt-4o-mini | $0.35 (40%) | 2 min |
| üéØ ST≈òEDN√ç | Batch size zv√Ω≈°en√≠ | 0 (jen rychlost) | 5 min |
| üí° N√çZK√Å | Selective KG | $2.16 (60%) | 1 hodina |
| üí° N√çZK√Å | Persistent cache | TBD | 2 hodiny |

**DOPORUƒåEN√Å AKCE:** Aplikovat Strategii A (MAXIM√ÅLN√ç √öSPORA) ‚Üí **87% √∫spora** za 10 minut pr√°ce

## üêõ AKTU√ÅLN√ç CHYBA (OPRAVENO)

**Error:** `max_tokens` parameter nen√≠ podporov√°n pro GPT-5/O-series modely
**Fix:** ‚úÖ Opraveno v `src/contextual_retrieval.py` a `src/summary_generator.py`
- Detekce GPT-5/O-series model≈Ø
- Pou≈æit√≠ `max_completion_tokens` m√≠sto `max_tokens`

## üìù POZN√ÅMKY

1. **Batching je ji≈æ aktivn√≠** ve v≈°ech f√°z√≠ch - dobr√©!
2. **Cache je aktivn√≠** pro embeddingy - dobr√©!
3. **Nejvƒõt≈°√≠ n√°klady:** Knowledge Graph (80% celku) - optimalizovat prioritnƒõ
4. **Nejjednodu≈°≈°√≠ √∫spora:** P≈ôepnout na BGE-M3 embeddings (100% √∫spora, 2 minuty)
5. **Trade-off:** Local embeddings (BGE-M3) jsou pomalej≈°√≠ p≈ôi prvn√≠m bƒõhu, ale pak stejnƒõ rychl√©

## üîç MONITORING

Pro sledov√°n√≠ n√°klad≈Ø pou≈æ√≠t:
```python
from src.cost_tracker import get_global_tracker

tracker = get_global_tracker()
print(tracker.get_summary())  # Detailn√≠ breakdown n√°klad≈Ø
```

V√Ωstup po ka≈æd√© indexaci automaticky zobrazuje:
- Celkov√© n√°klady
- Breakdown po operac√≠ch (summary, context, embedding, KG)
- Token usage per phase
